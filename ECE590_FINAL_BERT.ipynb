{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "#Sentiment analysis\n",
        "Fine-Tuning BERT\n",
        "ECE590 FINAL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spc_UH4B9xZl",
        "outputId": "32e3c3db-f782-486b-e167-da2d9a90400b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"llm_models\", exist_ok=True)\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, BertModel\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNIg8MIa5mLn"
      },
      "outputs": [],
      "source": [
        "!pip install datasets #needed for dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AfJRI72YOSy"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"dair-ai/emotion\")\n",
        "\n",
        "train_x = dataset['train']['text']\n",
        "train_y = dataset['train']['label']\n",
        "test_x = dataset['test']['text']\n",
        "test_y = dataset['test']['label']\n",
        "val_x = dataset['validation']['text']\n",
        "val_y = dataset['validation']['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6AndPQCwFlj"
      },
      "outputs": [],
      "source": [
        "len(train_x), len(val_x), len(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DFXeX5AqeDl"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HjxcMUuhqpY8"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_len= 128\n",
        "# Tokenize and encode the sentences\n",
        "X_train_encoded = tokenizer.batch_encode_plus(train_x,\n",
        "                                              add_special_tokens=True,\n",
        "                                              return_attention_mask=True,\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "L4T_oJJeu1dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_val_encoded = tokenizer.batch_encode_plus(val_x,\n",
        "                                              padding=True,\n",
        "                                              add_special_tokens=True,\n",
        "                                              return_attention_mask=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BqaA5mNyu2Do"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_test_encoded = tokenizer.batch_encode_plus(test_x,\n",
        "                                              add_special_tokens=True,\n",
        "                                              return_attention_mask=True,\n",
        "                                              padding=True,\n",
        "                                              truncation=True,\n",
        "                                              max_length = max_len,\n",
        "                                              return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_ZyZUOP7IHG"
      },
      "outputs": [],
      "source": [
        "#Show example\n",
        "k = 350\n",
        "print('Training Comments -->>',train_x[k])\n",
        "print('\\nInput Ids -->>\\n',X_train_encoded['input_ids'][k])\n",
        "print('\\nDecoded Ids -->>\\n',tokenizer.decode(X_train_encoded['input_ids'][k]))\n",
        "print('\\nAttention Mask -->>\\n',X_train_encoded['attention_mask'][k])\n",
        "print('\\nLabels -->>',train_y[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okdpIVlm7rfe"
      },
      "outputs": [],
      "source": [
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',problem_type=\"multi_label_classification\", num_labels=6)\n",
        "optimizer = tf.keras.optimizers.SGD(.01) #learning_rate=5e-05)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eDDNyBN7vD2"
      },
      "outputs": [],
      "source": [
        "#Train the model\n",
        "history = model.fit(\n",
        "\t[X_train_encoded['input_ids'], X_train_encoded['token_type_ids'], X_train_encoded['attention_mask']],\n",
        "\tpd.Series(train_y),\n",
        "\tvalidation_data=(\n",
        "\t[X_val_encoded['input_ids'], X_val_encoded['token_type_ids'], X_val_encoded['attention_mask']],pd.Series(val_y)),\n",
        "\tbatch_size=32,\n",
        "\tepochs=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro7h4rM4HJs-"
      },
      "outputs": [],
      "source": [
        "#Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    [X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']],\n",
        "    pd.Series(test_y)\n",
        ")\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYKtJhWGJbZL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict([X_test_encoded['input_ids'], X_test_encoded['token_type_ids'], X_test_encoded['attention_mask']])\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "metadata": {
        "id": "95xnclZZRo-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = preds.logits\n",
        "preds = np.argmax(logits, axis=1)\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "id": "PXJDmuE9VrPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PLOT CONFUSION MATRIX\n",
        "cm = confusion_matrix(test_y, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'])\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "NLK4cHbbVpLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PLOT LOSSES\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.title(\"BERT: Cross Entropy Loss by Epoch\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Mv6RsRYlrQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lLPL3RZOlqtO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}