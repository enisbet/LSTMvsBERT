{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhDUFBxt9xZg"
      },
      "source": [
        "# LSTM Model\n",
        "ECE590 Final\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Spc_UH4B9xZl"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "nltk.download('stopwords')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import os\n",
        "os.makedirs(\"llm_models\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets #needed for dataset"
      ],
      "metadata": {
        "id": "vNIg8MIa5mLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XODz_aDV9xZo"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"dair-ai/emotion\")"
      ],
      "metadata": {
        "id": "E8UukKpmBBQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "IRvCWNgSBFgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = dataset['train']['text']\n",
        "train_y = dataset['train']['label']\n",
        "test_x = dataset['test']['text']\n",
        "test_y = dataset['test']['label']\n",
        "val_x = dataset['validation']['text']\n",
        "val_y = dataset['validation']['label']"
      ],
      "metadata": {
        "id": "MQISxQssNzxY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_x), len(test_x), len(val_x))"
      ],
      "metadata": {
        "id": "6pMaG6eKBSXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYVH6t--9xZq"
      },
      "source": [
        "## Function to build a vocabulary based on the training corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sugI5VoJ9xZr"
      },
      "outputs": [],
      "source": [
        "def build_vocab(x_train:list, min_freq: int=20) -> dict:\n",
        "    \"\"\"\n",
        "    build a vocabulary based on the training corpus.\n",
        "    :param x_train:  List. Each sample in the list is a string of text.\n",
        "    :param min_freq: Int. The frequency threshold for selecting words.\n",
        "    :return: dictionary {word:index}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    word_list = []\n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word_list.append(word)\n",
        "\n",
        "    corpus = Counter(word_list)\n",
        "    corpus_ = [word for word, freq in corpus.items() if freq >= min_freq]\n",
        "    # creating a dict\n",
        "    vocab = {w:i+2 for i, w in enumerate(corpus_)}\n",
        "\n",
        "    #accomdate for padding and OOV tokens\n",
        "    vocab['<pad'] = 0\n",
        "    vocab['<unk>'] = 1\n",
        "    return vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca71G17F9xZt"
      },
      "source": [
        "## Tokenize Function.\n",
        "For each word in example, find its index in the vocabulary.\n",
        "Return a list of int that represents the indices of words in the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c6kj_qT69xZt"
      },
      "outputs": [],
      "source": [
        "def tokenize(vocab: dict, example: str)-> list:\n",
        "    \"\"\"\n",
        "    :param vocab: dict, the vocabulary.\n",
        "    :param example: a string of text.\n",
        "    :return: a list of token indices.\n",
        "    \"\"\"\n",
        "    token_inds = []\n",
        "\n",
        "    for word in example.lower().split():\n",
        "      try:\n",
        "        token_inds.append(vocab[word])\n",
        "      except:\n",
        "        token_inds.append(1) #unknown index\n",
        "    return token_inds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE\n",
        "vocab = build_vocab(train_x)\n",
        "tokenize(vocab, \"i feel burdened to share it\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBospwIMPDmt",
        "outputId": "67661cd1-ef37-467f-ad8e-e1033caf9e59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4, 43, 12, 268, 36]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ntSo4k9xZu"
      },
      "source": [
        "## Data Class - initialize and getitem\n",
        "- get item returns a dict of the tokenized review, the length of the review, and its corresponding label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2TDgA4p79xZu"
      },
      "outputs": [],
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, x, y, vocab, max_length=300) -> None:\n",
        "        \"\"\"\n",
        "        :param x: list of reviews\n",
        "        :param y: list of labels\n",
        "        :param vocab: vocabulary dictionary {word:index}.\n",
        "        :param max_length: the maximum sequence length.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        \"\"\"\n",
        "        Return the tokenized review and label by the given index.\n",
        "        :param idx: index of the sample.\n",
        "        :return: a dictionary containing three keys: 'ids', 'length', 'label'\n",
        "        \"\"\"\n",
        "        token_ids = tokenize(self.vocab, self.x[idx])\n",
        "        if self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "\n",
        "        return {\"ids\": token_ids, \"length\": len(token_ids), \"label\": self.y[idx]}\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.x)\n",
        "\n",
        "def collate(batch, pad_index):\n",
        "    batch_ids = [torch.LongTensor(i['ids']) for i in batch]\n",
        "    batch_ids = nn.utils.rnn.pad_sequence(batch_ids, padding_value=pad_index, batch_first=True)\n",
        "    batch_length = torch.Tensor([i['length'] for i in batch])\n",
        "\n",
        "    batch_label = torch.LongTensor([i['label'] for i in batch])\n",
        "    batch = {'ids': batch_ids, 'length': batch_length, 'label': batch_label}\n",
        "    return batch\n",
        "\n",
        "collate_fn = collate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zgSPYmf9xZv"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b9ofQ5R29xZv"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        hidden_dim: int,\n",
        "        output_dim: int,\n",
        "        n_layers: int,\n",
        "        dropout_rate: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout_rate, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim,)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, ids:torch.Tensor, length:torch.Tensor):\n",
        "        \"\"\"\n",
        "        Feed the given token ids to the model.\n",
        "        :param ids: [batch size, seq len] batch of token ids.\n",
        "        :param length: [batch size] batch of length of the token ids.\n",
        "        :return: prediction of size [batch size, output dim].\n",
        "        \"\"\"\n",
        "        embedded = self.dropout(self.embedding(ids))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=True,\n",
        "                                                            enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        hidden = self.dropout(hidden[-1])\n",
        "        out = self.fc(hidden)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBvNRJWwh_P"
      },
      "source": [
        "##Training, Validation, and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "13Sdl7MV9xZv"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader, model, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_losses = []\n",
        "    epoch_accs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm.tqdm(dataloader, desc='evaluating...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            epoch_losses.append(loss.item())\n",
        "            epoch_accs.append(accuracy.item())\n",
        "\n",
        "    return epoch_losses, epoch_accs\n",
        "\n",
        "def get_accuracy(prediction, label):\n",
        "    batch_size, _ = prediction.shape\n",
        "    predicted_classes = prediction.argmax(dim=-1)\n",
        "    correct_predictions = predicted_classes.eq(label).sum()\n",
        "    accuracy = correct_predictions / batch_size\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IDqtdeOJTk3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qXLkQSnS9xZw"
      },
      "outputs": [],
      "source": [
        "def train_and_test_model(vocab):\n",
        "    vocab_size = len(vocab)\n",
        "    print(\"Vocab Size = \", vocab_size)\n",
        "\n",
        "    train_data = Data(train_x, train_y, vocab, max_length = 300)\n",
        "    valid_data = Data(val_x, val_y, vocab, max_length = 300)\n",
        "    test_data = Data(test_x, test_y, vocab, max_length = 300)\n",
        "    batch_size = 32\n",
        "    collate = functools.partial(collate_fn, pad_index=0)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=collate, shuffle=True)\n",
        "    valid_dataloader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, collate_fn=collate)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate)\n",
        "\n",
        "    # Create Model\n",
        "    model = LSTM(\n",
        "        vocab_size,\n",
        "        embedding_dim=50,\n",
        "        hidden_dim=10,\n",
        "        output_dim=6,\n",
        "        n_layers=1,\n",
        "        dropout_rate=.2,\n",
        "        )\n",
        "\n",
        "    num_params = (sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    print(f'The model has {num_params:,} trainable parameters')\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr = .001) #optim.Adam(model.parameters(), lr=.0001)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "    #check accuracy, loss before training - should be about 1/6 acc\n",
        "    valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "    epoch_valid_loss = np.mean(valid_loss)\n",
        "    epoch_valid_acc = np.mean(valid_acc)\n",
        "    print(\"before training:\", epoch_valid_loss, epoch_valid_acc)\n",
        "\n",
        "\n",
        "    # Start training\n",
        "    all_train_l = []\n",
        "    all_val_l = []\n",
        "    best_valid_loss = float('inf')\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    valid_losses = []\n",
        "    valid_accs = []\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "\n",
        "        for batch in tqdm.tqdm(train_dataloader, desc='training...', file=sys.stdout):\n",
        "            ids = batch['ids'].to(device)\n",
        "            length = batch['length']\n",
        "            label = batch['label'].to(device)\n",
        "            prediction = model(ids, length)\n",
        "            loss = criterion(prediction, label)\n",
        "            accuracy = get_accuracy(prediction, label)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "            train_accs.append(accuracy.item())\n",
        "\n",
        "        epoch_train_loss = np.mean(train_losses)\n",
        "        all_train_l.append(epoch_train_loss)\n",
        "        epoch_train_acc = np.mean(train_accs)\n",
        "\n",
        "\n",
        "        valid_loss, valid_acc = evaluate(valid_dataloader, model, criterion, device)\n",
        "        epoch_valid_loss = np.mean(valid_loss)\n",
        "        all_val_l.append(epoch_valid_loss)\n",
        "        epoch_valid_acc = np.mean(valid_acc)\n",
        "\n",
        "        print(f'epoch: {epoch+1}')\n",
        "        print(f'train_loss: {epoch_train_loss:.3f}, train_acc: {epoch_train_acc:.3f}')\n",
        "        print(f'valid_loss: {epoch_valid_loss:.3f}, valid_acc: {epoch_valid_acc:.3f}')\n",
        "\n",
        "    test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "    epoch_test_loss = np.mean(test_loss)\n",
        "    epoch_test_acc = np.mean(test_acc)\n",
        "    print(f'test_loss: {epoch_test_loss:.3f}, test_acc: {epoch_test_acc:.3f}')\n",
        "\n",
        "    return model,all_train_l, all_val_l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab(train_x, min_freq= 10)"
      ],
      "metadata": {
        "id": "97IIdPH70OBL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzatRvfMwh_Q"
      },
      "outputs": [],
      "source": [
        "model, train_loss, val_loss = train_and_test_model(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PLOT LOSSES\n",
        "plt.plot([i for i in range(len(train_loss))], train_loss, label = \"train loss\")\n",
        "plt.plot([i for i in range(len(val_loss))], val_loss, label = \"val loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.title(\"LSTM: Cross Entropy Loss by Epoch\")\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "2HCpPFwSp0yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## COMPUTE CONFUSION MATRIX\n",
        "test_data = Data(test_x, test_y, vocab, max_length = 300)\n",
        "batch_size = 32\n",
        "collate = functools.partial(collate_fn, pad_index=0)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, collate_fn=collate)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "model.eval()\n",
        "preds = []\n",
        "#test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
        "with torch.no_grad():\n",
        "  for batch in tqdm.tqdm(test_dataloader, desc='evaluating...', file=sys.stdout):\n",
        "      ids = batch['ids'].to(device)\n",
        "      length = batch['length']\n",
        "      label = batch['label'].to(device)\n",
        "      prediction = model(ids, length)\n",
        "      batch_size, _ = prediction.shape\n",
        "      predicted_classes = prediction.argmax(dim=-1)\n",
        "      preds+=predicted_classes.cpu().numpy().tolist()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(test_y, preds,normalize='true'))\n"
      ],
      "metadata": {
        "id": "iDUzTADXq5nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "dSgJloBBPb0D"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}